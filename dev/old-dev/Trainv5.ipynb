{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "michael_zl_prime\r\n"
     ]
    }
   ],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Credentialed Accounts\r\n",
      "ACTIVE  ACCOUNT\r\n",
      "*       312164605303-compute@developer.gserviceaccount.com\r\n",
      "\r\n",
      "To set the active account, run:\r\n",
      "    $ gcloud config set account `ACCOUNT`\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test which virtualenv running in\n",
    "import sys\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 2.1.0a20191124 at http://instance-1:8889/ (Press CTRL+C to quit)\n",
      "E1129 11:21:48.206579 140479723202304 _internal.py:122] ::ffff:79.181.28.67 - - [29/Nov/2019 11:21:48] code 400, message Bad request syntax ('\\x16\\x03\\x01\\x02\\x00\\x01\\x00\\x01ü\\x03\\x03íúh\\'\\x9bèWq¬I\\x94Í\\x85\\x01@\\x0cß\\x85\\x86\\x15\\x15AJñ?X7\\xa0Åb+£ ¨Á\\x989WUA-£¤¯£\\x97\\x18\\x87ÍD\\x93\\x96Ü<Ý!¥xòÈÉW\\x07ã/\\x00\"\\x9a\\x9a\\x13\\x01\\x13\\x02\\x13\\x03À+À/À,À0Ì©Ì¨À\\x13À\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x00')\n",
      "E1129 11:21:49.409137 140479723202304 _internal.py:122] ::ffff:79.181.28.67 - - [29/Nov/2019 11:21:49] code 400, message Bad HTTP/0.9 request type ('\\x16\\x03\\x01\\x02\\x00\\x01\\x00\\x01ü\\x03\\x03¾»\\x05\\x9a\\x94\\x9e§m\\x83Y7\\x84\\xad$teì\\\\æãHÎ|çq/\\x8b\\\\¸%sQ')\n"
     ]
    }
   ],
   "source": [
    "#start tensor board\n",
    "!/usr/local/bin/tensorboard serve --logdir gs://dl_training_results/tensorboard --port 8889 --bind_all --purge_orphaned_data True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precautions to run on TPU.\n",
    "1.make sure you have a TPU, it's ON, correct version (2.x), correct IP\n",
    "2.for the checkpoints, make sure you have full scope access on the VM, service account is not enough.\n",
    "2.1.if permissions/scope have changed remove .gsutil cache 'rm -r ~/.gsutil'\n",
    "3.for the data buckets, make sure the TPU and VM have full permissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "from config import TRANSFORMED_TRAIN_ANNOTATIONS_PATH,TRANSFORMED_VALIDATION_ANNOTATIONS_PATH,IMAGE_SIZE\n",
    "from models.six_stage_linear_model import ModelMaker\n",
    "import dataset_functions\n",
    "import visualizations as v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0-dev20191124'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training config, can be moved later to main config\n",
    "CACHE=False\n",
    "CACHE_RAMFS=False #uses a ramfs file to force using main memory\n",
    "BATCH_SIZE=32  #must be small if caching\n",
    "SHUFFLE=True\n",
    "PREFETCH=10  #size of prefetch size, 0 to disable\n",
    "\n",
    "USING_TPU=False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Dataset side\n",
    "* add apropirate settings for TFRecordDataset -V\n",
    "* cache (before transformations) just to avoid disk access (should be ~9gigs) -V\n",
    "* move cache to ramfs -V\n",
    "* add augmentation ,after cache, probably before transformations \n",
    "* add prefetch, shuffle -V \n",
    "* create validation dataset -V\n",
    "\n",
    "Compilation side\n",
    "* callbacks:\n",
    "    *tensorboard -V\n",
    "    *saving weights every epoch -V\n",
    "    *learning rate\n",
    "    *\n",
    "* add tensorboard callback -V\n",
    "\n",
    "* add hyper parameters (learning rate, learning rate decay)\n",
    "* Figure out metrics (accuracy)\n",
    "    *add recall ,mean absoulte error, and 'island' recall error -V\n",
    "    *sort metrics by stage -V\n",
    "* add validation -V\n",
    "\n",
    "All\n",
    "* add comments\n",
    "\n",
    "TPUs\n",
    "* try with TPUs -VVV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results\n",
    "*caching seems to eat only gpu memory\n",
    "*Intersting to try a ramfs or a BytesIO object , (ramfs is probably better though as tf handles the interface)\n",
    "*This should allow to increase batch size\n",
    "-Full epoc currently is 1:15h\n",
    "TPUs\n",
    "-After figuring out TPUs, 13min epoch is achieved!\n",
    "\n",
    "-execution adjustments, batch siez, caching.\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_TPU:\n",
    "    tpu_ip='10.0.3.2'\n",
    "    # !!!MAKE SURE THE TPU ADDRESS IS CORRECT!!\n",
    "    # 1.ip must be correct\n",
    "    # 2.tpu must be turned on!\n",
    "    # 3.version must be 'nightly-2.x'\n",
    "    # 4.tpu must be reachable (check with gce netowrking/connectivity test)\n",
    "    # if not this will hang\n",
    "    tpu_address = 'grpc://'+tpu_ip+'8470'\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Caching using ramfs (irrelevant for tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CACHE_RAMFS:\n",
    "    !mkdir /tmp/ramdisk\n",
    "    !sudo umount /tmp/ramdisk\n",
    "    !sudo mount -t ramfs -o size=512m ramfs /tmp/ramdisk\n",
    "    !sudo chown $LOGNAME:$LOGNAME /tmp/ramdisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CACHE_RAMFS:\n",
    "    cache_loc=\"/tmp/ramdisk/cache_t\"\n",
    "    cache_v_loc=\"/tmp/ramdisk/cache_v\"\n",
    "else:\n",
    "    cache_loc=None\n",
    "    cache_v_loc=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transformer=dataset_functions.LabelTransformer()\n",
    "@tf.function\n",
    "def make_label_tensors(elem):\n",
    "    \"\"\"Transforms a dict data element:\n",
    "    1.Read jpg to tensor \n",
    "    1.1 Resize img to correct size for network\n",
    "    2.Convert keypoints to correct form label tensor\n",
    "    3.Convert joints to correct form label tensor\n",
    "    outputs a tuple data element\"\"\"\n",
    "    \n",
    "    idd=elem['id']\n",
    "    kpt_tr=label_transformer.keypoints_spots_vmapfn(elem['kpts'])\n",
    "    paf_tr=label_transformer.joints_PAFs(elem['joints'])\n",
    "    \n",
    "    image_raw=elem[\"image_raw\"]\n",
    "    image=tf.image.decode_jpeg(image_raw,channels=3)\n",
    "    image=tf.image.convert_image_dtype(image,dtype=tf.float32)\n",
    "    image=tf.image.resize(image,IMAGE_SIZE)\n",
    "    return image,(paf_tr,kpt_tr),idd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def place_training_labels(image,labels,idd):\n",
    "    \"\"\"Disterbutes labels into the correct configuration for the model, ie 4 PAF stage, 2 kpt stages\n",
    "    must match the model\"\"\"\n",
    "    paf_tr=labels[0]\n",
    "    kpt_tr=labels[1]\n",
    "    return image,(paf_tr,paf_tr,paf_tr,paf_tr,kpt_tr,kpt_tr) #this should match the model outputs, and is different for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and Parse the TFrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE=56000 #exact size not critical\n",
    "DATASET_VAL_SIZE=2500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "figure out GCS storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix=TRANSFORMED_TRAIN_ANNOTATIONS_PATH.split(os.sep)[-1]\n",
    "val_prefix=TRANSFORMED_VALIDATION_ANNOTATIONS_PATH.split(os.sep)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name=\"datasets_bucket_a\"\n",
    "gs_prefix=\"gs://\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client() #must have apropriate authenitication to work \n",
    "\n",
    "train_blobs = storage_client.list_blobs(bucket_name,prefix=train_prefix)\n",
    "val_blobs = storage_client.list_blobs(bucket_name,prefix=val_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_files_train=[gs_prefix+bucket_name+'/'+blob.name for blob in train_blobs]\n",
    "tfrecord_files_val=[gs_prefix+bucket_name+'/'+blob.name for blob in val_blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfrecord_files_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TFRecordDataset(tfrecord_files_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_parser=dataset_functions.TFrecordParser() #used for \n",
    "\n",
    "#order of transformations is critical!\n",
    "\n",
    "#TFrecord files to raw format\n",
    "ds = tf.data.TFRecordDataset(tfrecord_files_train) #numf reads can be put here, but I don't think I/O is the bottleneck\n",
    "\n",
    "#raw format to imgs,tensors(coords kpts)\n",
    "ds=ds.map(TF_parser.read_tfrecord)\n",
    "\n",
    "#cache  ,caching is here before decompressing jpgs and label tensors (should be ~9GB) , (full dataset should be ~90, cache later if RAM aviable)\n",
    "if CACHE: ds=ds.cache(cache_loc)\n",
    "if SHUFFLE: ds=ds.shuffle(100)    \n",
    "    \n",
    "#Augmentation should be here, to operate on smaller tensors\n",
    "    \n",
    "#imgs,tensors to label_tensors (46,46,17/38)\n",
    "ds=ds.map(make_label_tensors)\n",
    "#imgs,label_tensors arrange for model outputs\n",
    "ds=ds.map(place_training_labels) \n",
    "\n",
    "#batch\n",
    "ds=ds.batch(BATCH_SIZE)\n",
    "#repeat\n",
    "ds=ds.repeat()\n",
    "#prefetch\n",
    "if PREFETCH: ds=ds.prefetch(PREFETCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_parser=dataset_functions.TFrecordParser() #used for \n",
    "\n",
    "#order of transformations is critical!\n",
    "\n",
    "#TFrecord files to raw format\n",
    "ds_v = tf.data.TFRecordDataset(tfrecord_files_val) #numf reads can be put here, but I don't think I/O is the bottleneck\n",
    "#raw format to imgs,tensors(coords kpts)\n",
    "ds_v=ds_v.map(TF_parser.read_tfrecord)   \n",
    "\n",
    "#cache  \n",
    "if CACHE: ds_v=ds_v.cache(cache_v_loc)\n",
    "    \n",
    "#imgs,tensors to label_tensors (46,46,17/38)\n",
    "ds_v=ds_v.map(make_label_tensors)\n",
    "#imgs,label_tensors arrange for model outputs\n",
    "ds_v=ds_v.map(place_training_labels) \n",
    "#batch\n",
    "ds_v=ds_v.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=next(iter(ds))\n",
    "#st\n",
    "#st_v=next(iter(ds_v))\n",
    "#v.show_pafs_kpts_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(st[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 368, 368, 3), ((None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 17), (None, 46, 46, 17))), types: (tf.float32, (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 368, 368, 3), ((None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 17), (None, 46, 46, 17))), types: (tf.float32, (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#st[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Callbacks\n",
    "**Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local dir for CPU/GPU training\n",
    "#checkpoints_dir='./checkpoints'\n",
    "#!mkdir checkpoints\n",
    "#gcs dir for TPU training\n",
    "checkpoints_dir='gs://dl_training_results/checkpoints'\n",
    "#make sure the directory exists, if not make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now=datetime.datetime.now().strftime(\"%a%d%m%y-%H%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://dl_training_results/checkpoints/ModelWeights-Wed271119-1403-{epoch:04d}.ckpt'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=datetime.datetime.now()\n",
    "now=t.strftime(\"%a%d%m%y-%H%M\")\n",
    "checkpoints_path = checkpoints_dir+\"/ModelWeights-\"+datetime.datetime.now().strftime(\"%a%d%m%y-%H%M\")+\"-{epoch:04d}.ckpt\" \n",
    "checkpoints_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/checkpoints/test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "#if this fails, the checkpointing won't work\n",
    "!touch /tmp/test\n",
    "!gsutil cp /tmp/test {checkpoints_dir}/test\n",
    "!gsutil rm {checkpoints_dir}/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoints_path,\n",
    "                                             save_weights_only=True,\n",
    "                                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_dir='gs://dl_training_results/logging/'\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(logging_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/logging//test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "#if this fails, the checkpointing won't work\n",
    "!touch /tmp/test\n",
    "!gsutil cp /tmp/test {logging_dir}/test\n",
    "!gsutil rm {logging_dir}/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorBoard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir='gs://dl_training_results/tensorboard/'+datetime.datetime.now().strftime(\"%a%d%m%y-%H%M\")\n",
    "tensorboard_callback=tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=tensorboard_dir\n",
    "    ,update_freq=5000 #to update sooner than every epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_training_results/tensorboard/Wed271119-1403/test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "#if this fails, the checkpointing won't work\n",
    "!touch /tmp/test\n",
    "!gsutil cp /tmp/test {tensorboard_dir}/test\n",
    "!gsutil rm {tensorboard_dir}/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the command to run tensorboard\n",
    "#tensorboard serve --logdir gs://dl_training_results/tensorboard --port 8889 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mse_2d_loss(y_true, y_pred):\n",
    "    pixel_losses=tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    return tf.math.reduce_mean(pixel_losses,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalogRecall(tf.keras.metrics.Metric):\n",
    "    \"\"\"This metric returns the overlap of the true gaussian 'islands' and the predicted ones\"\"\"\n",
    "    def __init__(self, name='analog_recall_error',thershold=0.01, **kwargs):\n",
    "        super(AnalogRecall, self).__init__(name=name, **kwargs)\n",
    "        self.mean = self.add_weight(name='mean', initializer='zeros')\n",
    "        self.thershold=thershold\n",
    "    \n",
    "    \n",
    "    def update_state(self, y_true, y_pred,**kwargs):     \n",
    "        \n",
    "        true_island_sum=tf.reduce_sum(tf.where(y_true>self.thershold,y_true,0)) #get sum of the true island\n",
    "        true_island_size=tf.cast(tf.math.count_nonzero(y_true>self.thershold),dtype=tf.float32) #get size of the island   \n",
    "        mean_island_true=true_island_sum/true_island_size #average island value\n",
    "               \n",
    "        err=y_true-y_pred  #get all error\n",
    "        recall_err=tf.where(err>0,err,0)  #get only recall error, the parts where prediction is missing\n",
    "        recall_err_sum=tf.reduce_sum(recall_err)        \n",
    "        err_island_size=tf.cast(tf.math.count_nonzero(y_true>self.thershold),tf.float32)  #get size of the islands above thershold\n",
    "        \n",
    "        mean_island_recall_err=recall_err_sum/err_island_size  #mean of the error \n",
    "        \n",
    "        value=1-mean_island_recall_err/mean_island_true #the 1- converts it to recall accuracy onstead of err        \n",
    "        self.mean.assign_add(value)\n",
    "        \n",
    "    def result(self):\n",
    "        return self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define():\n",
    "    model_maker=ModelMaker()\n",
    "    train_model,test_model=model_maker.create_models()\n",
    "    \n",
    "    #this must match the model output order\n",
    "    metrics={'stage1paf_output':  [AnalogRecall(),tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,'stage2paf_output':  [AnalogRecall(),tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,'stage3paf_output':  [AnalogRecall(),tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,'stage4paf_output':  [AnalogRecall(),tf.keras.metrics.MeanAbsoluteError()]\n",
    "         ,'stage5heatmap_output': [AnalogRecall(),tf.keras.metrics.MeanAbsoluteError()]    \n",
    "         ,'stage6heatmap_output': [AnalogRecall(),tf.keras.metrics.MeanAbsoluteError()]\n",
    "        }\n",
    "    \n",
    "    train_model.compile(optimizer=tf.keras.optimizers.Adam()\n",
    "                    ,loss=mse_2d_loss\n",
    "                    ,metrics=metrics                    \n",
    "                   )\n",
    "\n",
    "if USING_TPU:\n",
    "    with strategy.scope():\n",
    "        define()\n",
    "else:\n",
    "    define()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint: gs://dl_training_results/checkpoints/ModelWeights-Wed271119-1356-0002.ckpt\n"
     ]
    }
   ],
   "source": [
    "#load latest weights\n",
    "latest = tf.train.latest_checkpoint(checkpoints_dir)\n",
    "if latest:\n",
    "    print(\"Found checkpoint: %s\" % latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9302a7ed437f4e28941d8d8bb6aadca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Load last weights')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load=widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Load last weights'\n",
    ")\n",
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest and (load.value):\n",
    "    train_model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=int(DATASET_SIZE/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7 steps\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function mse_2d_loss at 0x7ff2058f9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function mse_2d_loss at 0x7ff2058f9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 6:21 - loss: 0.2048 - stage1paf_output_loss: 0.0429 - stage2paf_output_loss: 0.0260 - stage3paf_output_loss: 0.0169 - stage4paf_output_loss: 0.0065 - stage5heatmap_output_loss: 0.0427 - stage6heatmap_output_loss: 0.0697 - stage1paf_output_analog_recall_error: 1.8624 - stage1paf_output_mean_absolute_error: 0.1273 - stage2paf_output_analog_recall_error: 1.5509 - stage2paf_output_mean_absolute_error: 0.0916 - stage3paf_output_analog_recall_error: 1.1052 - stage3paf_output_mean_absolute_error: 0.0634 - stage4paf_output_analog_recall_error: 1.3757 - stage4paf_output_mean_absolute_error: 0.0422 - stage5heatmap_output_analog_recall_error: 1.4097 - stage5heatmap_output_mean_absolute_error: 0.1144 - stage6heatmap_output_analog_recall_error: 2.6775 - stage6heatmap_output_mean_absolute_error: 0.1736WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.591561). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.591561). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/7 [=======>......................] - ETA: 2:45 - loss: 0.1926 - stage1paf_output_loss: 0.0256 - stage2paf_output_loss: 0.0313 - stage3paf_output_loss: 0.0240 - stage4paf_output_loss: 0.0052 - stage5heatmap_output_loss: 0.0446 - stage6heatmap_output_loss: 0.0618 - stage1paf_output_analog_recall_error: 4.0064 - stage1paf_output_mean_absolute_error: 0.0872 - stage2paf_output_analog_recall_error: 2.0914 - stage2paf_output_mean_absolute_error: 0.0742 - stage3paf_output_analog_recall_error: 3.4491 - stage3paf_output_mean_absolute_error: 0.0723 - stage4paf_output_analog_recall_error: 1.7662 - stage4paf_output_mean_absolute_error: 0.0286 - stage5heatmap_output_analog_recall_error: 2.3495 - stage5heatmap_output_mean_absolute_error: 0.1092 - stage6heatmap_output_analog_recall_error: 3.8450 - stage6heatmap_output_mean_absolute_error: 0.1464WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.792061). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.792061). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/7 [===========>..................] - ETA: 1:29 - loss: 0.1696 - stage1paf_output_loss: 0.0176 - stage2paf_output_loss: 0.0213 - stage3paf_output_loss: 0.0164 - stage4paf_output_loss: 0.0042 - stage5heatmap_output_loss: 0.0437 - stage6heatmap_output_loss: 0.0665 - stage1paf_output_analog_recall_error: 4.2879 - stage1paf_output_mean_absolute_error: 0.0619 - stage2paf_output_analog_recall_error: 2.1810 - stage2paf_output_mean_absolute_error: 0.0513 - stage3paf_output_analog_recall_error: 3.5646 - stage3paf_output_mean_absolute_error: 0.0501 - stage4paf_output_analog_recall_error: 2.1133 - stage4paf_output_mean_absolute_error: 0.0242 - stage5heatmap_output_analog_recall_error: 2.4708 - stage5heatmap_output_mean_absolute_error: 0.0990 - stage6heatmap_output_analog_recall_error: 5.3018 - stage6heatmap_output_mean_absolute_error: 0.1467WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.407508). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.407508). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 [========================>.....] - ETA: 11s - loss: 0.1295 - stage1paf_output_loss: 0.0092 - stage2paf_output_loss: 0.0110 - stage3paf_output_loss: 0.0086 - stage4paf_output_loss: 0.0025 - stage5heatmap_output_loss: 0.0430 - stage6heatmap_output_loss: 0.0551 - stage1paf_output_analog_recall_error: 4.3614 - stage1paf_output_mean_absolute_error: 0.0322 - stage2paf_output_analog_recall_error: 2.2232 - stage2paf_output_mean_absolute_error: 0.0266 - stage3paf_output_analog_recall_error: 3.5862 - stage3paf_output_mean_absolute_error: 0.0260 - stage4paf_output_analog_recall_error: 2.1946 - stage4paf_output_mean_absolute_error: 0.0131 - stage5heatmap_output_analog_recall_error: 3.7756 - stage5heatmap_output_mean_absolute_error: 0.0938 - stage6heatmap_output_analog_recall_error: 5.2142 - stage6heatmap_output_mean_absolute_error: 0.1126WARNING:tensorflow:5 out of the last 13 calls to <function mse_2d_loss at 0x7ff2058f9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function mse_2d_loss at 0x7ff2058f9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to gs://dl_training_results/checkpoints/ModelWeights-Wed271119-1403-0001.ckpt\n",
      "7/7 [==============================] - 128s 18s/step - loss: 0.1236 - stage1paf_output_loss: 0.0080 - stage2paf_output_loss: 0.0096 - stage3paf_output_loss: 0.0075 - stage4paf_output_loss: 0.0023 - stage5heatmap_output_loss: 0.0428 - stage6heatmap_output_loss: 0.0535 - stage1paf_output_analog_recall_error: 4.3630 - stage1paf_output_mean_absolute_error: 0.0278 - stage2paf_output_analog_recall_error: 2.2510 - stage2paf_output_mean_absolute_error: 0.0230 - stage3paf_output_analog_recall_error: 3.5925 - stage3paf_output_mean_absolute_error: 0.0225 - stage4paf_output_analog_recall_error: 2.2114 - stage4paf_output_mean_absolute_error: 0.0115 - stage5heatmap_output_analog_recall_error: 4.3779 - stage5heatmap_output_mean_absolute_error: 0.0934 - stage6heatmap_output_analog_recall_error: 5.2040 - stage6heatmap_output_mean_absolute_error: 0.1080 - val_loss: 0.0847 - val_stage1paf_output_loss: 7.7630e-04 - val_stage2paf_output_loss: 7.7640e-04 - val_stage3paf_output_loss: 7.7615e-04 - val_stage4paf_output_loss: 7.7624e-04 - val_stage5heatmap_output_loss: 0.0401 - val_stage6heatmap_output_loss: 0.0415 - val_stage1paf_output_analog_recall_error: nan - val_stage1paf_output_mean_absolute_error: 0.0014 - val_stage2paf_output_analog_recall_error: nan - val_stage2paf_output_mean_absolute_error: 0.0014 - val_stage3paf_output_analog_recall_error: nan - val_stage3paf_output_mean_absolute_error: 0.0014 - val_stage4paf_output_analog_recall_error: nan - val_stage4paf_output_mean_absolute_error: 0.0015 - val_stage5heatmap_output_analog_recall_error: nan - val_stage5heatmap_output_mean_absolute_error: 0.0976 - val_stage6heatmap_output_analog_recall_error: nan - val_stage6heatmap_output_mean_absolute_error: 0.0776\n",
      "Epoch 2/2\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.703389). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.703389). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 8s - loss: 0.0987 - stage1paf_output_loss: 7.1053e-04 - stage2paf_output_loss: 7.1072e-04 - stage3paf_output_loss: 7.1035e-04 - stage4paf_output_loss: 7.1048e-04 - stage5heatmap_output_loss: 0.0467 - stage6heatmap_output_loss: 0.0492 - stage1paf_output_analog_recall_error: -0.0176 - stage1paf_output_mean_absolute_error: 0.0014 - stage2paf_output_analog_recall_error: -0.0128 - stage2paf_output_mean_absolute_error: 0.0014 - stage3paf_output_analog_recall_error: -0.0154 - stage3paf_output_mean_absolute_error: 0.0014 - stage4paf_output_analog_recall_error: -0.0086 - stage4paf_output_mean_absolute_error: 0.0014 - stage5heatmap_output_analog_recall_error: 0.9052 - stage5heatmap_output_mean_absolute_error: 0.1069 - stage6heatmap_output_analog_recall_error: 0.0422 - stage6heatmap_output_mean_absolute_error: 0.0885WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.358219). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.358219). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0876 - stage1paf_output_loss: 7.5189e-04 - stage2paf_output_loss: 7.5281e-04 - stage3paf_output_loss: 7.5178e-04 - stage4paf_output_loss: 7.5175e-04 - stage5heatmap_output_loss: 0.0414 - stage6heatmap_output_loss: 0.0432 - stage1paf_output_analog_recall_error: -0.0671 - stage1paf_output_mean_absolute_error: 0.0014 - stage2paf_output_analog_recall_error: 0.0024 - stage2paf_output_mean_absolute_error: 0.0015 - stage3paf_output_analog_recall_error: -0.0648 - stage3paf_output_mean_absolute_error: 0.0014 - stage4paf_output_analog_recall_error: -0.0466 - stage4paf_output_mean_absolute_error: 0.0014 - stage5heatmap_output_analog_recall_error: 3.6529 - stage5heatmap_output_mean_absolute_error: 0.0915 - stage6heatmap_output_analog_recall_error: 0.2867 - stage6heatmap_output_mean_absolute_error: 0.0808\n",
      "Epoch 00002: saving model to gs://dl_training_results/checkpoints/ModelWeights-Wed271119-1403-0002.ckpt\n",
      "7/7 [==============================] - 50s 7s/step - loss: 0.0878 - stage1paf_output_loss: 7.6029e-04 - stage2paf_output_loss: 7.6106e-04 - stage3paf_output_loss: 7.6017e-04 - stage4paf_output_loss: 7.6014e-04 - stage5heatmap_output_loss: 0.0415 - stage6heatmap_output_loss: 0.0433 - stage1paf_output_analog_recall_error: -0.0799 - stage1paf_output_mean_absolute_error: 0.0014 - stage2paf_output_analog_recall_error: -0.0105 - stage2paf_output_mean_absolute_error: 0.0014 - stage3paf_output_analog_recall_error: -0.0777 - stage3paf_output_mean_absolute_error: 0.0014 - stage4paf_output_analog_recall_error: -0.0594 - stage4paf_output_mean_absolute_error: 0.0014 - stage5heatmap_output_analog_recall_error: 4.0307 - stage5heatmap_output_mean_absolute_error: 0.0906 - stage6heatmap_output_analog_recall_error: 0.2911 - stage6heatmap_output_mean_absolute_error: 0.0806 - val_loss: 0.0842 - val_stage1paf_output_loss: 7.7603e-04 - val_stage2paf_output_loss: 7.7580e-04 - val_stage3paf_output_loss: 7.7580e-04 - val_stage4paf_output_loss: 7.7580e-04 - val_stage5heatmap_output_loss: 0.0396 - val_stage6heatmap_output_loss: 0.0415 - val_stage1paf_output_analog_recall_error: nan - val_stage1paf_output_mean_absolute_error: 0.0013 - val_stage2paf_output_analog_recall_error: nan - val_stage2paf_output_mean_absolute_error: 0.0013 - val_stage3paf_output_analog_recall_error: nan - val_stage3paf_output_mean_absolute_error: 0.0013 - val_stage4paf_output_analog_recall_error: nan - val_stage4paf_output_mean_absolute_error: 0.0013 - val_stage5heatmap_output_analog_recall_error: nan - val_stage5heatmap_output_mean_absolute_error: 0.0883 - val_stage6heatmap_output_analog_recall_error: nan - val_stage6heatmap_output_mean_absolute_error: 0.0767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff20453e198>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.fit(\n",
    "    ds,epochs=2\n",
    "    ,steps_per_epoch=7\n",
    "    ,validation_data=ds_v\n",
    "    ,callbacks=[cp_callback\n",
    "                #,csv_logger\n",
    "                ,tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v.show_pafs_kpts_img(img.numpy(),paf.numpy(),kpt.numpy(),1,1) #can be used to draw the tensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!!!IT IS FUCKING WORKING!!!!!!!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mean absolute error is probably best measure of overall error, but is bound to be very small.\n",
    "recall can work for kpts but not for PAFs, accuracy is useless."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
