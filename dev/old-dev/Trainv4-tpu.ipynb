{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test which virtualenv running in\n",
    "import sys\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precautions to run on TPU.\n",
    "1.make sure you have a TPU, it's ON, correct version (2.x), correct IP\n",
    "2.for the checkpoints, make sure you have full scope access on the VM, service account is not enough.\n",
    "2.1.if permissions/scope have changed remove .gsutil cache 'rm -r ~/.gsutil'\n",
    "3.for the data buckets, make sure the TPU and VM have full permissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "from config import TRANSFORMED_TRAIN_ANNOTATIONS_PATH,TRANSFORMED_VALIDATION_ANNOTATIONS_PATH,IMAGE_SIZE\n",
    "from models.six_stage_linear_model import ModelMaker\n",
    "import dataset_functions\n",
    "import visualizations as v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0-dev20191124'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training config, can be moved later to main config\n",
    "CACHE=False\n",
    "CACHE_RAMFS=False #uses a ramfs file to force using main memory\n",
    "BATCH_SIZE=32  #must be small if caching\n",
    "SHUFFLE=True\n",
    "PREFETCH=10  #size of prefetch size, 0 to disable"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO\n",
    "Dataset side\n",
    "* add apropirate settings for TFRecordDataset -V\n",
    "* cache (before transformations) just to avoid disk access (should be ~9gigs) -V\n",
    "* move cache to ramfs -V\n",
    "* add augmentation ,after cache, probably before transformations \n",
    "* add prefetch, shuffle -V \n",
    "* create validation dataset -V\n",
    "\n",
    "Compilation side\n",
    "* callbacks:\n",
    "    *tensorboard\n",
    "    *saving weights every epoch -V\n",
    "    *learning rate\n",
    "    *\n",
    "* add tensorboard callback\n",
    "\n",
    "* add hyper parameters (learning rate, learning rate decay)\n",
    "* add metrics (accuracy)\n",
    "* add validation -V\n",
    "\n",
    "All\n",
    "* add comments\n",
    "\n",
    "TPUs\n",
    "* try with TPUs -VVV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results\n",
    "*caching seems to eat only gpu memory\n",
    "*Intersting to try a ramfs or a BytesIO object , (ramfs is probably better though as tf handles the interface)\n",
    "*This should allow to increase batch size\n",
    "-Full epoc currently is 1:15h\n",
    "TPUs\n",
    "-After figuring out TPUs, 13min epoch is achieved!\n",
    "\n",
    "-execution adjustments, batch siez, caching.\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: 10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: 10.0.3.2:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "tpu_ip='10.0.3.2'\n",
    "# !!!MAKE SURE THE TPU ADDRESS IS CORRECT!!\n",
    "# 1.ip must be correct\n",
    "# 2.tpu must be turned on!\n",
    "# 3.version must be 'nightly-2.x'\n",
    "# 4.tpu must be reachable (check with gce netowrking/connectivity test)\n",
    "# if not this will hang\n",
    "tpu_address = 'grpc://'+tpu_ip+':8470'\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Caching using ramfs (irrelevant for tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CACHE_RAMFS:\n",
    "    !mkdir /tmp/ramdisk\n",
    "    !sudo umount /tmp/ramdisk\n",
    "    !sudo mount -t ramfs -o size=512m ramfs /tmp/ramdisk\n",
    "    !sudo chown $LOGNAME:$LOGNAME /tmp/ramdisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CACHE_RAMFS:\n",
    "    cache_loc=\"/tmp/ramdisk/cache_t\"\n",
    "    cache_v_loc=\"/tmp/ramdisk/cache_v\"\n",
    "else:\n",
    "    cache_loc=None\n",
    "    cache_v_loc=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transformer=dataset_functions.LabelTransformer()\n",
    "@tf.function\n",
    "def make_label_tensors(elem):\n",
    "    \"\"\"Transforms a dict data element:\n",
    "    1.Read jpg to tensor \n",
    "    1.1 Resize img to correct size for network\n",
    "    2.Convert keypoints to correct form label tensor\n",
    "    3.Convert joints to correct form label tensor\n",
    "    outputs a tuple data element\"\"\"\n",
    "    \n",
    "    idd=elem['id']\n",
    "    kpt_tr=label_transformer.keypoints_spots_vmapfn(elem['kpts'])\n",
    "    paf_tr=label_transformer.joints_PAFs(elem['joints'])\n",
    "    \n",
    "    image_raw=elem[\"image_raw\"]\n",
    "    image=tf.image.decode_jpeg(image_raw,channels=3)\n",
    "    image=tf.image.convert_image_dtype(image,dtype=tf.float32)\n",
    "    image=tf.image.resize(image,IMAGE_SIZE)\n",
    "    return image,(paf_tr,kpt_tr),idd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def place_training_labels(image,labels,idd):\n",
    "    \"\"\"Disterbutes labels into the correct configuration for the model, ie 4 PAF stage, 2 kpt stages\n",
    "    must match the model\"\"\"\n",
    "    paf_tr=labels[0]\n",
    "    kpt_tr=labels[1]\n",
    "    return image,(paf_tr,paf_tr,paf_tr,paf_tr,kpt_tr,kpt_tr) #this should match the model outputs, and is different for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and Parse the TFrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE=56000 #exact size not critical\n",
    "DATASET_VAL_SIZE=2500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "figure out GCS storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix=TRANSFORMED_TRAIN_ANNOTATIONS_PATH.split(os.sep)[-1]\n",
    "val_prefix=TRANSFORMED_VALIDATION_ANNOTATIONS_PATH.split(os.sep)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name=\"datasets_bucket_a\"\n",
    "gs_prefix=\"gs://\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client() #must have apropriate authenitication to work \n",
    "\n",
    "train_blobs = storage_client.list_blobs(bucket_name,prefix=train_prefix)\n",
    "val_blobs = storage_client.list_blobs(bucket_name,prefix=val_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_files_train=[gs_prefix+bucket_name+'/'+blob.name for blob in train_blobs]\n",
    "tfrecord_files_val=[gs_prefix+bucket_name+'/'+blob.name for blob in val_blobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfrecord_files_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TFRecordDataset(tfrecord_files_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_parser=dataset_functions.TFrecordParser() #used for \n",
    "\n",
    "#order of transformations is critical!\n",
    "\n",
    "#TFrecord files to raw format\n",
    "ds = tf.data.TFRecordDataset(tfrecord_files_train) #numf reads can be put here, but I don't think I/O is the bottleneck\n",
    "\n",
    "#raw format to imgs,tensors(coords kpts)\n",
    "ds=ds.map(TF_parser.read_tfrecord)\n",
    "\n",
    "#cache  ,caching is here before decompressing jpgs and label tensors (should be ~9GB) , (full dataset should be ~90, cache later if RAM aviable)\n",
    "if CACHE: ds=ds.cache(cache_loc)\n",
    "if SHUFFLE: ds=ds.shuffle(100)    \n",
    "    \n",
    "#Augmentation should be here, to operate on smaller tensors\n",
    "    \n",
    "#imgs,tensors to label_tensors (46,46,17/38)\n",
    "ds=ds.map(make_label_tensors)\n",
    "#imgs,label_tensors arrange for model outputs\n",
    "ds=ds.map(place_training_labels) \n",
    "\n",
    "#batch\n",
    "ds=ds.batch(BATCH_SIZE)\n",
    "#repeat\n",
    "ds=ds.repeat()\n",
    "#prefetch\n",
    "if PREFETCH: ds=ds.prefetch(PREFETCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_parser=dataset_functions.TFrecordParser() #used for \n",
    "\n",
    "#order of transformations is critical!\n",
    "\n",
    "#TFrecord files to raw format\n",
    "ds_v = tf.data.TFRecordDataset(tfrecord_files_val) #numf reads can be put here, but I don't think I/O is the bottleneck\n",
    "#raw format to imgs,tensors(coords kpts)\n",
    "ds_v=ds_v.map(TF_parser.read_tfrecord)   \n",
    "\n",
    "#cache  \n",
    "if CACHE: ds_v=ds_v.cache(cache_v_loc)\n",
    "    \n",
    "#imgs,tensors to label_tensors (46,46,17/38)\n",
    "ds_v=ds_v.map(make_label_tensors)\n",
    "#imgs,label_tensors arrange for model outputs\n",
    "ds_v=ds_v.map(place_training_labels) \n",
    "#batch\n",
    "ds_v=ds_v.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=next(iter(ds))\n",
    "#st\n",
    "#st_v=next(iter(ds_v))\n",
    "#v.show_pafs_kpts_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(st[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 368, 368, 3), ((None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 17), (None, 46, 46, 17))), types: (tf.float32, (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 368, 368, 3), ((None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 38), (None, 46, 46, 17), (None, 46, 46, 17))), types: (tf.float32, (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#st[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local dir for CPU/GPU training\n",
    "#checkpoints_dir='./checkpoints'\n",
    "#!mkdir checkpoints\n",
    "#gcs dir for TPU training\n",
    "checkpoints_dir='gs://dl_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tue261119-2108'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "t=datetime.datetime.now()\n",
    "now=t.strftime(\"%a%d%m%y-%H%M\")\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://dl_checkpoints/ModelWeights-Tue261119-2108-{epoch:04d}.ckpt'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints_path = checkpoints_dir+\"/ModelWeights-\"+now+\"-{epoch:04d}.ckpt\" \n",
    "checkpoints_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    0.0 B/    0.0 B]                                                \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://dl_checkpoints/test...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "#if this fails, the checkpointing won't work\n",
    "!touch /tmp/test\n",
    "!gsutil cp /tmp/test {checkpoints_dir}/test\n",
    "!gsutil rm {checkpoints_dir}/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoints_path,\n",
    "                                             save_weights_only=True,\n",
    "                                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mse_2d_loss(y_true, y_pred):\n",
    "    pixel_losses=tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    return tf.math.reduce_mean(pixel_losses,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model_maker=ModelMaker()\n",
    "    train_model,test_model=model_maker.create_models()\n",
    "    \n",
    "    train_model.compile(optimizer=tf.keras.optimizers.Adam()\n",
    "                    ,loss=mse_2d_loss\n",
    "                    ,metrics=[\"acc\"]                    \n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint: gs://dl_checkpoints/ModelWeights-Tue:26:11:19-21:03-0002.ckpt\n"
     ]
    }
   ],
   "source": [
    "#load latest weights\n",
    "latest = tf.train.latest_checkpoint(checkpoints_dir)\n",
    "if latest:\n",
    "    print(\"Found checkpoint: %s\" % latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43399b135a7f44109f9937e71e36e115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Load last weights')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load=widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Load last weights'\n",
    ")\n",
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest and (load.value):\n",
    "    train_model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=int(DATASET_SIZE/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7 steps\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function mse_2d_loss at 0x7f2cf3515d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function mse_2d_loss at 0x7f2cf3515d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 [========================>.....] - ETA: 10s - loss: 0.1549 - stage1paf_output_loss: 0.0083 - stage2paf_output_loss: 0.0097 - stage3paf_output_loss: 0.0091 - stage4paf_output_loss: 0.0122 - stage5heatmap_output_loss: 0.0490 - stage6heatmap_output_loss: 0.0667 - stage1paf_output_acc: 0.0091 - stage2paf_output_acc: 0.1919 - stage3paf_output_acc: 0.1335 - stage4paf_output_acc: 0.0022 - stage5heatmap_output_acc: 0.0463 - stage6heatmap_output_acc: 0.0458WARNING:tensorflow:5 out of the last 13 calls to <function mse_2d_loss at 0x7f2cf3515d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function mse_2d_loss at 0x7f2cf3515d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to gs://dl_checkpoints/ModelWeights-Tue:26:11:19-21:03-0001.ckpt\n",
      "7/7 [==============================] - 110s 16s/step - loss: 0.1468 - stage1paf_output_loss: 0.0072 - stage2paf_output_loss: 0.0085 - stage3paf_output_loss: 0.0079 - stage4paf_output_loss: 0.0106 - stage5heatmap_output_loss: 0.0487 - stage6heatmap_output_loss: 0.0640 - stage1paf_output_acc: 0.0112 - stage2paf_output_acc: 0.2882 - stage3paf_output_acc: 0.2333 - stage4paf_output_acc: 0.1182 - stage5heatmap_output_acc: 0.0467 - stage6heatmap_output_acc: 0.0456 - val_loss: 0.0839 - val_stage1paf_output_loss: 7.7557e-04 - val_stage2paf_output_loss: 7.7591e-04 - val_stage3paf_output_loss: 7.7590e-04 - val_stage4paf_output_loss: 7.7603e-04 - val_stage5heatmap_output_loss: 0.0393 - val_stage6heatmap_output_loss: 0.0414 - val_stage1paf_output_acc: 0.4582 - val_stage2paf_output_acc: 0.8990 - val_stage3paf_output_acc: 0.8930 - val_stage4paf_output_acc: 0.9000 - val_stage5heatmap_output_acc: 0.0416 - val_stage6heatmap_output_acc: 0.0388\n",
      "Epoch 2/2\n",
      "6/7 [========================>.....] - ETA: 1s - loss: 0.0807 - stage1paf_output_loss: 7.1335e-04 - stage2paf_output_loss: 7.1314e-04 - stage3paf_output_loss: 7.1312e-04 - stage4paf_output_loss: 7.1316e-04 - stage5heatmap_output_loss: 0.0378 - stage6heatmap_output_loss: 0.0400 - stage1paf_output_acc: 0.3738 - stage2paf_output_acc: 0.9083 - stage3paf_output_acc: 0.9063 - stage4paf_output_acc: 0.9090 - stage5heatmap_output_acc: 0.0399 - stage6heatmap_output_acc: 0.0412\n",
      "Epoch 00002: saving model to gs://dl_checkpoints/ModelWeights-Tue:26:11:19-21:03-0002.ckpt\n",
      "7/7 [==============================] - 50s 7s/step - loss: 0.0796 - stage1paf_output_loss: 7.3465e-04 - stage2paf_output_loss: 7.3448e-04 - stage3paf_output_loss: 7.3445e-04 - stage4paf_output_loss: 7.3449e-04 - stage5heatmap_output_loss: 0.0373 - stage6heatmap_output_loss: 0.0394 - stage1paf_output_acc: 0.4520 - stage2paf_output_acc: 0.9135 - stage3paf_output_acc: 0.9126 - stage4paf_output_acc: 0.9149 - stage5heatmap_output_acc: 0.0380 - stage6heatmap_output_acc: 0.0418 - val_loss: 0.0823 - val_stage1paf_output_loss: 7.7580e-04 - val_stage2paf_output_loss: 7.7585e-04 - val_stage3paf_output_loss: 7.7580e-04 - val_stage4paf_output_loss: 7.7580e-04 - val_stage5heatmap_output_loss: 0.0384 - val_stage6heatmap_output_loss: 0.0409 - val_stage1paf_output_acc: 0.9160 - val_stage2paf_output_acc: 0.9382 - val_stage3paf_output_acc: 0.9423 - val_stage4paf_output_acc: 0.9413 - val_stage5heatmap_output_acc: 0.0403 - val_stage6heatmap_output_acc: 0.0499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2cdc0cfe80>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.fit(\n",
    "    ds,epochs=2\n",
    "    ,steps_per_epoch=7\n",
    "    ,validation_data=ds_v\n",
    "    ,callbacks=[cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v.show_pafs_kpts_img(img.numpy(),paf.numpy(),kpt.numpy(),1,1) #can be used to draw the tensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!!!IT IS FUCKING WORKING!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Lambda(lambda x: x, input_shape=(10,10,5)))\n",
    "\n",
    "model.compile(loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tf.zeros((1,10,10,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state([1, 2, 3, 4], [0, 2, 3, 4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
